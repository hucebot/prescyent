{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a MLP Predictor on a Synthetic Dataset\n",
    "\n",
    "Here we test and plot a MLP predictor on the Synthetic Circle Cluster Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "from prescyent.dataset import SCCDataset, SCCDatasetConfig\n",
    "\n",
    "xs = list(range(0, 16, 4))\n",
    "ys = list(range(0, 16, 4))\n",
    "clusters = list(product(xs, ys))\n",
    "num_clusters = len(clusters)\n",
    "cluster_radius = 1\n",
    "cluster_num_traj = 12\n",
    "\n",
    "num_trajs = [cluster_num_traj for _ in range(num_clusters)]\n",
    "radius = [cluster_radius for _ in range(num_clusters)]\n",
    "starting_xs = [x_value for x_value, _ in  clusters]\n",
    "starting_ys = [y_value for _ , y_value in  clusters]\n",
    "\n",
    "dataset_config = SCCDatasetConfig(\n",
    "    # seed=5,  # if none, the seed is random\n",
    "    frequency=20,\n",
    "    future_size=10,\n",
    "    history_size=20,\n",
    "    radius_eps=0.2,\n",
    "    perturbation_range=0.2,\n",
    "    num_perturbation_points=10,\n",
    "    num_points=100,\n",
    "    num_trajs=num_trajs,\n",
    "    radius=radius,\n",
    "    starting_xs=starting_xs,\n",
    "    starting_ys=starting_ys,\n",
    ")\n",
    "dataset = SCCDataset(dataset_config)\n",
    "\n",
    "dataset.plot_traj(dataset.trajectories.test[0])\n",
    "dataset.plot_trajectories_dim_wise(dataset.trajectories.test, title=\"All test trajectories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from prescyent.predictor import MlpPredictor, MlpConfig, ConstantPredictor, PredictorConfig, TrainingConfig\n",
    "from prescyent.scaler import ScalerConfig\n",
    "from prescyent.utils.enums import Scalers, TrajectoryDimensions, LossFunctions\n",
    "\n",
    "# -- Init scaler\n",
    "scaler_config = ScalerConfig(\n",
    "    do_feature_wise_scaling=True,\n",
    "    scaler=Scalers.STANDARDIZATION,\n",
    "    scaling_axis=TrajectoryDimensions.TEMPORAL,\n",
    ")\n",
    "# -- Init predictor\n",
    "print(\"Initializing predictor...\", end=\" \")\n",
    "config = MlpConfig(\n",
    "    dataset_config=dataset_config,\n",
    "    context_size=dataset.context_size_sum,\n",
    "    scaler_config=scaler_config,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    deriv_on_last_frame=True,\n",
    "    loss_fn=LossFunctions.MTDLOSS,\n",
    ")\n",
    "predictor = MlpPredictor(config=config)\n",
    "print(\"OK\")\n",
    "\n",
    "# Train\n",
    "training_config = TrainingConfig(\n",
    "    max_epochs=200,  # Maximum number of training epochs\n",
    "    devices=\"auto\",  # Chose the best available devices (see lightning documentation for more)\n",
    "    accelerator=\"auto\",  # Chose the best available accelerator (see lightning documentation for more)\n",
    "    lr=0.0001,  # The learning rate\n",
    "    early_stopping_patience=10,  # We'll stop the training before max_epochs if the validation loss doesn't improve for 10 epochs\n",
    ")\n",
    "\n",
    "# Scaler is also trained by the predictor's method !\n",
    "predictor.train(dataset, training_config)\n",
    "\n",
    "# Save the predictor\n",
    "xp_dir = (\n",
    "    Path(\"data\")\n",
    "    / \"models\"\n",
    "    / f\"{dataset.DATASET_NAME}\"\n",
    "    / f\"h{dataset_config.history_size}_f{dataset_config.future_size}_{dataset.frequency}hz\"\n",
    ")\n",
    "model_dir = xp_dir / f\"{predictor.name}\" / f\"version_{predictor.version}\"\n",
    "print(\"Model directory:\", model_dir)\n",
    "predictor.save(model_dir, rm_log_path=False)\n",
    "# We can save also the dataset config so that we can load it later if needed\n",
    "dataset.save_config(model_dir / \"dataset_config.json\")\n",
    "\n",
    "# Test predictor over the test set so that we know how good we are\n",
    "predictor.test(dataset)\n",
    "\n",
    "# Compare with delayed baseline\n",
    "delayed_config = PredictorConfig(\n",
    "    dataset_config=dataset_config, save_path=f\"{xp_dir}\"\n",
    ")\n",
    "delayed = ConstantPredictor(config=delayed_config)\n",
    "delayed.test(dataset)\n",
    "\n",
    "print(\n",
    "    \"You can visualize all logs from this script at xp_dir using tensorboard like this:\"\n",
    ")\n",
    "print(f\"tensorboard --logdir {xp_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
